# LocalRAG Configuration
# Copy this file to .env and edit as needed

# Mode: "local" (Ollama) or "cloud" (OpenAI/Anthropic)
LOCALRAG_MODE=local

# LLM model (for local: any Ollama model; for cloud: gpt-4o-mini, etc.)
LOCALRAG_LLM_MODEL=llama3.2

# Embedding model
LOCALRAG_EMBED_MODEL=nomic-embed-text

# Generation settings
LOCALRAG_TEMPERATURE=0.1
LOCALRAG_MAX_TOKENS=1024

# Chunking
LOCALRAG_CHUNK_SIZE=512
LOCALRAG_CHUNK_OVERLAP=50

# Retrieval
LOCALRAG_TOP_K=5

# Storage paths
LOCALRAG_CHROMA_PATH=./data/chroma
LOCALRAG_UPLOAD_PATH=./data/uploads

# Ollama (local mode)
LOCALRAG_OLLAMA_BASE_URL=http://localhost:11434

# Cloud API keys (only needed when LOCALRAG_MODE=cloud)
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=sk-ant-...
