<p align="center">
  <img src="docs/assets/localrag-banner.png" alt="LocalRAG" width="600"/>
</p>

<h1 align="center">LocalRAG</h1>

<p align="center">
  <strong>Privacy-first document intelligence. Your data never leaves your machine.</strong>
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#api-reference">API</a> â€¢
  <a href="#roadmap">Roadmap</a> â€¢
  <a href="#contributing">Contributing</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/python-3.10%2B-blue?style=flat-square" alt="Python 3.10+"/>
  <img src="https://img.shields.io/badge/license-MIT-green?style=flat-square" alt="MIT License"/>
  <img src="https://img.shields.io/badge/PRs-welcome-brightgreen?style=flat-square" alt="PRs Welcome"/>
  <img src="https://img.shields.io/badge/build%20in%20public-ðŸ”¨-orange?style=flat-square" alt="Build in Public"/>
</p>

---

## The Problem

Most RAG (Retrieval-Augmented Generation) solutions send your documents to third-party cloud services. For sensitive data â€” legal contracts, medical records, financial reports â€” that's a non-starter.

**LocalRAG** is a privacy-first document Q&A system that processes everything locally by default, with optional cloud LLM integration when privacy requirements allow it.

Upload documents â†’ Ask questions â†’ Get cited answers. **No data leaves your machine unless you explicitly choose cloud mode.**

## Features

- ðŸ”’ **Privacy-first** â€” All document processing, chunking, and embedding happens locally
- ðŸ“„ **Multi-format ingestion** â€” PDF, DOCX, TXT, Markdown, CSV support
- ðŸ” **Hybrid search** â€” Combines semantic (vector) + keyword (BM25) retrieval for better accuracy
- ðŸ§  **Flexible LLM backend** â€” Use Ollama (local) or OpenAI/Anthropic (cloud) â€” your choice
- ðŸ“Š **Source citations** â€” Every answer includes references to specific document chunks
- âš¡ **FastAPI backend** â€” Production-ready REST API with async support
- ðŸ“ˆ **Built-in evaluation** â€” Measure retrieval accuracy and answer quality with RAGAS
- ðŸ³ **Docker ready** â€” One command to run the entire stack

## Quick Start

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.ai) (for local LLM mode)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/localrag.git
cd localrag

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Pull a local model (optional, for fully local mode)
ollama pull llama3.2
ollama pull nomic-embed-text
```

### Run LocalRAG

```bash
# Start the API server
python -m localrag.api.main

# Server runs at http://localhost:8000
# Interactive docs at http://localhost:8000/docs
```

### Basic Usage

```python
from localrag import LocalRAG

# Initialize with local mode (default)
rag = LocalRAG(mode="local")

# Ingest documents
rag.ingest("./documents/")

# Ask questions
answer = rag.query("What are the key terms in the contract?")
print(answer.text)
print(answer.sources)  # Document chunks with page numbers
```

### API Usage

```bash
# Upload a document
curl -X POST http://localhost:8000/api/v1/documents/upload \
  -F "file=@contract.pdf"

# Query your documents
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the payment terms?", "top_k": 5}'
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI Server                       â”‚
â”‚                   (REST API + WebSocket)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ingestion   â”‚  â”‚  Retrieval   â”‚  â”‚  Generation    â”‚  â”‚
â”‚  â”‚  Pipeline    â”‚  â”‚  Engine      â”‚  â”‚  Layer         â”‚  â”‚
â”‚  â”‚             â”‚  â”‚              â”‚  â”‚                â”‚  â”‚
â”‚  â”‚ â€¢ PDF Parse â”‚  â”‚ â€¢ Semantic   â”‚  â”‚ â€¢ Ollama       â”‚  â”‚
â”‚  â”‚ â€¢ DOCX Parseâ”‚  â”‚ â€¢ BM25       â”‚  â”‚ â€¢ OpenAI       â”‚  â”‚
â”‚  â”‚ â€¢ Chunking  â”‚  â”‚ â€¢ Hybrid     â”‚  â”‚ â€¢ Anthropic    â”‚  â”‚
â”‚  â”‚ â€¢ Cleaning  â”‚  â”‚ â€¢ Re-ranking â”‚  â”‚ â€¢ Prompt Mgmt  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                â”‚                   â”‚           â”‚
â”‚         â–¼                â–¼                   â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              ChromaDB (Vector Store)                 â”‚ â”‚
â”‚  â”‚           Local persistent storage                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           Evaluation Module (RAGAS)                 â”‚ â”‚
â”‚  â”‚    Retrieval accuracy â€¢ Answer quality â€¢ Faithfulnessâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
localrag/
â”œâ”€â”€ localrag/                  # Core package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ ingestion/             # Document processing pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ parsers.py         # PDF, DOCX, TXT parsers
â”‚   â”‚   â”œâ”€â”€ chunker.py         # Semantic chunking strategies
â”‚   â”‚   â””â”€â”€ preprocessor.py    # Text cleaning & normalization
â”‚   â”œâ”€â”€ retrieval/             # Search & retrieval
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vectorstore.py     # ChromaDB integration
â”‚   â”‚   â”œâ”€â”€ bm25.py            # Keyword search
â”‚   â”‚   â”œâ”€â”€ hybrid.py          # Hybrid search orchestration
â”‚   â”‚   â””â”€â”€ reranker.py        # Cross-encoder re-ranking
â”‚   â”œâ”€â”€ llm/                   # LLM abstraction layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py            # Base LLM interface
â”‚   â”‚   â”œâ”€â”€ ollama_client.py   # Local inference via Ollama
â”‚   â”‚   â”œâ”€â”€ openai_client.py   # OpenAI API client
â”‚   â”‚   â””â”€â”€ prompts.py         # Prompt templates
â”‚   â”œâ”€â”€ api/                   # FastAPI application
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py            # App entry point
â”‚   â”‚   â”œâ”€â”€ routes/            # API route handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py
â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â””â”€â”€ models.py          # Pydantic request/response models
â”‚   â”œâ”€â”€ evaluation/            # Quality measurement
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py         # RAGAS integration
â”‚   â””â”€â”€ utils/                 # Shared utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logging.py
â”œâ”€â”€ tests/                     # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”œâ”€â”€ docs/                      # Documentation
â”‚   â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ guides/
â”œâ”€â”€ scripts/                   # Helper scripts
â”‚   â””â”€â”€ seed_data.py
â”œâ”€â”€ docker/                    # Docker configuration
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ Makefile
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## API Reference

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/documents/upload` | Upload and ingest a document |
| `GET` | `/api/v1/documents` | List all ingested documents |
| `DELETE` | `/api/v1/documents/{id}` | Remove a document |
| `POST` | `/api/v1/query` | Ask a question across your documents |
| `POST` | `/api/v1/query/stream` | Stream a response (SSE) |
| `GET` | `/api/v1/health` | Health check |
| `GET` | `/api/v1/stats` | Collection statistics |

## Configuration

LocalRAG uses environment variables for configuration. Copy `.env.example` to `.env`:

```bash
cp .env.example .env
```

| Variable | Default | Description |
|----------|---------|-------------|
| `LOCALRAG_MODE` | `local` | `local` (Ollama) or `cloud` (OpenAI/Anthropic) |
| `LOCALRAG_LLM_MODEL` | `llama3.2` | Model name for generation |
| `LOCALRAG_EMBED_MODEL` | `nomic-embed-text` | Model for embeddings |
| `LOCALRAG_CHUNK_SIZE` | `512` | Token count per chunk |
| `LOCALRAG_CHUNK_OVERLAP` | `50` | Overlap between chunks |
| `LOCALRAG_TOP_K` | `5` | Number of chunks to retrieve |
| `LOCALRAG_CHROMA_PATH` | `./data/chroma` | ChromaDB storage path |
| `OPENAI_API_KEY` | â€” | Required only for cloud mode |

## Roadmap

### Phase 1: Core (Weeks 1-4) â† **Current**
- [x] Project structure & configuration
- [ ] Document ingestion pipeline (PDF, DOCX, TXT)
- [ ] Semantic chunking with overlap
- [ ] ChromaDB vector store integration
- [ ] Basic RAG query pipeline
- [ ] FastAPI REST endpoints
- [ ] Ollama integration for local inference

### Phase 2: Production-Grade (Weeks 5-8)
- [ ] Hybrid search (semantic + BM25)
- [ ] Cross-encoder re-ranking
- [ ] Streaming responses (SSE)
- [ ] Docker containerization
- [ ] RAGAS evaluation pipeline
- [ ] Multi-modal support (tables, images from PDFs)

### Phase 3: Enterprise Features (Weeks 9-12)
- [ ] LangChain agent integration
- [ ] Multi-tenancy support
- [ ] Authentication & audit logging
- [ ] Async document processing queue
- [ ] Comprehensive benchmarks vs. cloud RAG solutions
- [ ] CLI tool for terminal-based Q&A

## Performance

> Benchmarks coming in Phase 2. Will compare retrieval accuracy, latency, and answer quality against cloud-based RAG solutions.

## Built With

- **[FastAPI](https://fastapi.tiangolo.com/)** â€” High-performance async API framework
- **[LangChain](https://python.langchain.com/)** â€” LLM orchestration and chain management
- **[ChromaDB](https://www.trychroma.com/)** â€” Open-source vector database
- **[Ollama](https://ollama.ai/)** â€” Local LLM inference
- **[RAGAS](https://docs.ragas.io/)** â€” RAG evaluation framework

## Contributing

Contributions are welcome! Please read the [Contributing Guide](CONTRIBUTING.md) before submitting a PR.

```bash
# Setup development environment
pip install -r requirements-dev.txt
pre-commit install

# Run tests
make test

# Run linting
make lint
```

## License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <strong>Building this in public.</strong> Follow along on <a href="https://linkedin.com/in/yourprofile">LinkedIn</a> for daily updates.
</p>
